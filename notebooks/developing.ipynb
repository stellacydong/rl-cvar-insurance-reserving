{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0db82360-c4ad-4377-a7ba-06b6d5fd276c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WorkersComp | Seed 0 | Level 0...\n",
      "Training WorkersComp | Seed 0 | Level 1...\n",
      "Training WorkersComp | Seed 0 | Level 2...\n",
      "Training WorkersComp | Seed 0 | Level 3...\n",
      "Training WorkersComp | Seed 42 | Level 0...\n",
      "Training WorkersComp | Seed 42 | Level 1...\n",
      "Training WorkersComp | Seed 42 | Level 2...\n",
      "Training WorkersComp | Seed 42 | Level 3...\n",
      "Training OtherLiability | Seed 0 | Level 0...\n",
      "Training OtherLiability | Seed 0 | Level 1...\n",
      "Training OtherLiability | Seed 0 | Level 2...\n",
      "Training OtherLiability | Seed 0 | Level 3...\n",
      "Training OtherLiability | Seed 42 | Level 0...\n",
      "Training OtherLiability | Seed 42 | Level 1...\n",
      "Training OtherLiability | Seed 42 | Level 2...\n",
      "Training OtherLiability | Seed 42 | Level 3...\n"
     ]
    }
   ],
   "source": [
    "# Final RL Reserving Framework with Regime Breakdown, Stress Testing, and Regime-Aware Curriculum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# ========== Data Loading ==========\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    possible_cols = [\n",
    "        (\"IncurLoss_D\", \"CumPaidLoss_D\", \"PostedReserve97_D\"),\n",
    "        (\"IncurLoss_h1\", \"CumPaidLoss_h1\", \"PostedReserve97_h1\"),\n",
    "        (\"IncurredLosses\", \"PaidLosses\", \"Reserves\")\n",
    "    ]\n",
    "    for incur, paid, res in possible_cols:\n",
    "        if all(c in df.columns for c in [incur, paid, res]):\n",
    "            df = df.rename(columns={incur: \"IncurredLosses\", paid: \"PaidLosses\", res: \"Reserves\"})\n",
    "            break\n",
    "    else:\n",
    "        raise KeyError(\"No valid columns found.\")\n",
    "    df = df[[\"AccidentYear\", \"DevelopmentYear\", \"IncurredLosses\", \"PaidLosses\", \"Reserves\"]].dropna()\n",
    "    scaler = MinMaxScaler()\n",
    "    df[[\"IncurredLosses\", \"PaidLosses\", \"Reserves\"]] = scaler.fit_transform(df[[\"IncurredLosses\", \"PaidLosses\", \"Reserves\"]])\n",
    "    return df\n",
    "\n",
    "# ========== Environment ==========\n",
    "class ReservingEnv(gym.Env):\n",
    "    def __init__(self, df, level=0, max_level=3, seed=None, fixed_shock=None):\n",
    "        super().__init__()\n",
    "        self.df_original = df.copy()\n",
    "        self.level = level\n",
    "        self.max_level = max_level\n",
    "        self.seed_value = seed if seed is not None else random.randint(0, 9999)\n",
    "        self.fixed_shock = fixed_shock\n",
    "        self._setup_env()\n",
    "        self.action_space = spaces.Discrete(7)\n",
    "        self.action_mapping = np.linspace(-0.1, 0.1, 7)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(9,), dtype=np.float32)\n",
    "        self.violation_memory = 0.0\n",
    "\n",
    "    def _setup_env(self):\n",
    "        np.random.seed(self.seed_value)\n",
    "        frac = 0.4 + 0.6 * (self.level / self.max_level)\n",
    "        n = int(len(self.df_original) * frac)\n",
    "        self.df = self.df_original.sample(n=n, replace=True).reset_index(drop=True)\n",
    "        noise = np.random.normal(0, 0.01 + 0.01 * self.level, size=(n, 3))\n",
    "        self.df[[\"Reserves\", \"PaidLosses\", \"IncurredLosses\"]] += noise\n",
    "        self.df[\"Volatility\"] = self.df[[\"PaidLosses\", \"IncurredLosses\"]].std(axis=1)\n",
    "        self.df[\"Capital\"] = 1 - abs(self.df[\"Reserves\"] - self.df[\"IncurredLosses\"])\n",
    "\n",
    "        if self.fixed_shock is not None:\n",
    "            self.df[\"MacroShock\"] = self.fixed_shock\n",
    "        else:\n",
    "            regime_map = {0: (1.0, 0.1), 1: (1.2, 0.2), 2: (1.5, 0.3), 3: (1.8, 0.4)}\n",
    "            mean, std = regime_map.get(self.level, (1.0, 0.1))\n",
    "            shock = np.random.normal(loc=mean, scale=std, size=n)\n",
    "            self.df[\"MacroShock\"] = np.clip(shock, 0.8, 2.0)\n",
    "\n",
    "        self.df[\"Volatility\"] *= self.df[\"MacroShock\"]\n",
    "        self.max_steps = min(60 + 10 * self.level, len(self.df))\n",
    "        self.index = 0\n",
    "        self.shortfall_history = []\n",
    "        self.violations = 0\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self._setup_env()\n",
    "        self.violation_memory = 0.0\n",
    "        self.shortfall_history.clear()\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        row = self.df.iloc[self.index]\n",
    "        return np.array([\n",
    "            row[\"Reserves\"], row[\"PaidLosses\"], row[\"IncurredLosses\"],\n",
    "            row[\"DevelopmentYear\"], row[\"Volatility\"], row[\"Capital\"],\n",
    "            self.violation_memory, self.level / self.max_level, row[\"MacroShock\"]\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        row = self.df.iloc[self.index]\n",
    "        reserve_adj = self.action_mapping[action]\n",
    "        reserve = np.clip(row[\"Reserves\"] + reserve_adj, 0, 1)\n",
    "        incurred = row[\"IncurredLosses\"]\n",
    "        volatility = row[\"Volatility\"]\n",
    "\n",
    "        min_reg = 0.4 + 0.2 * volatility\n",
    "        buffer_zone = min_reg + 0.05\n",
    "        shortfall = max(0, incurred - reserve)\n",
    "        self.shortfall_history.append(shortfall)\n",
    "        cvar_threshold = int(90 + 5 * (volatility + 0.1))\n",
    "        cvar = np.percentile(self.shortfall_history, cvar_threshold) if self.shortfall_history else 0.0\n",
    "\n",
    "        reg_penalty = 0\n",
    "        if reserve < min_reg:\n",
    "            reg_penalty = (min_reg - reserve) * 100.0\n",
    "            self.violations += 1\n",
    "        elif reserve < buffer_zone:\n",
    "            reg_penalty = (buffer_zone - reserve) * 10.0\n",
    "\n",
    "        norm_shortfall = np.clip(shortfall / 0.2, 0, 1)\n",
    "        norm_cvar = np.clip(cvar / 0.2, 0, 1)\n",
    "        norm_capital = 1 - row[\"Capital\"]\n",
    "\n",
    "        reward = -(\n",
    "            2.0 * norm_shortfall + 4.0 * norm_cvar + 1.0 * norm_capital + reg_penalty\n",
    "        )\n",
    "\n",
    "        self.violation_memory = 0.95 * self.violation_memory + 0.05 * (1 if reserve < min_reg else 0)\n",
    "        self.index += 1\n",
    "        done = self.index >= self.max_steps\n",
    "        return self._get_obs(), reward, done, False, {}\n",
    "\n",
    "# ========== Evaluation ==========\n",
    "def evaluate_agent(env, model, episodes=5):\n",
    "    rewards, shortfalls, violations, regime_stats = [], [], [], []\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total = 0\n",
    "        sf_list = []\n",
    "        v_count = 0\n",
    "        regime = \"High\" if obs[-1] > 1.5 else \"Moderate\" if obs[-1] > 1.0 else \"Low\"\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = env.step(int(action))\n",
    "            row = env.df.iloc[env.index - 1]\n",
    "            sf_list.append(max(0, row[\"IncurredLosses\"] - row[\"Reserves\"]))\n",
    "            if row[\"Reserves\"] < (0.4 + 0.2 * row[\"Volatility\"]):\n",
    "                v_count += 1\n",
    "            total += reward\n",
    "\n",
    "        rewards.append(total)\n",
    "        shortfalls.append(np.mean(sf_list))\n",
    "        violations.append(v_count / env.max_steps)\n",
    "        regime_stats.append(regime)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"Reward\": rewards, \"Shortfall\": shortfalls, \"CVaR95\": shortfalls,\n",
    "        \"ViolationRate\": violations, \"Regime\": regime_stats\n",
    "    })\n",
    "    return df.groupby(\"Regime\").mean().reset_index()\n",
    "\n",
    "# ========== Training ==========\n",
    "def train_curriculum(df, seeds, lob_name, max_level=3):\n",
    "    all_logs = []\n",
    "    for seed in seeds:\n",
    "        for level in range(max_level + 1):\n",
    "            print(f\"Training {lob_name} | Seed {seed} | Level {level}...\")\n",
    "            env = DummyVecEnv([lambda: ReservingEnv(df, level=level, max_level=max_level, seed=seed)])\n",
    "            model = PPO(\"MlpPolicy\", env, verbose=0, seed=seed)\n",
    "            model.learn(total_timesteps=15000)\n",
    "\n",
    "            eval_env = ReservingEnv(df, level=level, seed=seed)\n",
    "            level_results = evaluate_agent(eval_env, model, episodes=5)\n",
    "            level_results.insert(0, \"Level\", level)\n",
    "            level_results.insert(0, \"Seed\", seed)\n",
    "            level_results.insert(0, \"LOB\", lob_name)\n",
    "            all_logs.append(level_results)\n",
    "\n",
    "            # Optional stress test\n",
    "            for shock in [0.8, 1.0, 1.5, 2.0]:\n",
    "                stress_env = ReservingEnv(df, level=level, seed=seed, fixed_shock=shock)\n",
    "                stress_stats = evaluate_agent(stress_env, model, episodes=3)\n",
    "                stress_stats.insert(0, \"MacroShock\", shock)\n",
    "                stress_stats.insert(0, \"StressTest\", True)\n",
    "                stress_stats.insert(0, \"Level\", level)\n",
    "                stress_stats.insert(0, \"Seed\", seed)\n",
    "                stress_stats.insert(0, \"LOB\", lob_name)\n",
    "                all_logs.append(stress_stats)\n",
    "\n",
    "    final_df = pd.concat(all_logs, ignore_index=True)\n",
    "    final_df.to_csv(f\"final_regime_results_{lob_name}.csv\", index=False)\n",
    "    return final_df\n",
    "\n",
    "# ========== Entry Point ==========\n",
    "if __name__ == \"__main__\":\n",
    "    df_wk = load_data(\"wkcomp_pos.csv\")\n",
    "    df_oth = load_data(\"othliab_pos.csv\")\n",
    "\n",
    "    train_curriculum(df_wk, seeds=[0, 42], lob_name=\"WorkersComp\")\n",
    "    train_curriculum(df_oth, seeds=[0, 42], lob_name=\"OtherLiability\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63917c56-2fdf-470e-897a-bad4cebd29f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
